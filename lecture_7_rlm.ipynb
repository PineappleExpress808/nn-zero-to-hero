{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "795f55f7",
   "metadata": {},
   "source": [
    "##### Build a LM that will produce Shakespeare-like prose. \n",
    "  \n",
    "`Lecture`\n",
    "\n",
    "https://www.youtube.com/watch?v=kCc8FmEb1nY&t=14s\n",
    "\n",
    "`Workflow` \n",
    "\n",
    "https://www.youtube.com/watch?v=-j6y-5t37os&t=1s running notebooks in VSCode:\n",
    "\n",
    "* Initiate VENV in Terminal\n",
    "* Start Jupyter Notebook\n",
    "* Copy url of the server into VSCode (server button on bottom-middle)\n",
    "* Select Kernal: Python 3 (ipykernel) for Jupyter session\n",
    "\n",
    "`Dataset` - \n",
    "\n",
    "The Shakespeare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498cd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "vocab size (unique chars):  65\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))\n",
    "\n",
    "# Chars\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"vocab size (unique chars): \", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdccccfb",
   "metadata": {},
   "source": [
    "`Tokenization` -\n",
    "\n",
    "Mapping chars to a sting of int.\n",
    "\n",
    "It is common to do sub-words.\n",
    "\n",
    "E.g., Google uses `SentencePiece`\n",
    "\n",
    "`TikToken (OAI)` \n",
    "* Uses sub-words\n",
    "* ~50k tokens (https://news.ycombinator.com/item?id=34008839), a larger codebook size \n",
    "* Also see HuggingFace tokenizers: https://huggingface.co/docs/tokenizers/quicktour\n",
    "* Note, using gpt2 encoding `Hi There` is encoded to 2 tokens between `1 - 50257`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d28e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n",
      "[17250, 1318]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "print(enc.n_vocab)\n",
    "print(enc.encode('Hi There'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7a6e8cf",
   "metadata": {},
   "source": [
    "`Ours`\n",
    "* We use a char-level tokenizer\n",
    "* Small number of tokens, a small codebook\n",
    "* With char encoding `Hi There` is encoded to 8 tokens between `1 - 65`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4426a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 47, 1, 32, 46, 43, 56, 43]\n",
      "Hi There\n"
     ]
    }
   ],
   "source": [
    "# char mapping\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# encode and decode\n",
    "encode = lambda s: [stoi[c] for c in s] # encode a string as a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decode a list of integers as a string\n",
    "\n",
    "print(encode(\"Hi There\"))\n",
    "print(decode(encode(\"Hi There\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23d6962e",
   "metadata": {},
   "source": [
    "`Dataset`\n",
    "\n",
    "The works of Shakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9760f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Encode dataset and wrap in Torch tensor\n",
    "import torch, numpy as np\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape,data.dtype)\n",
    " \n",
    "# Create splits\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f19d6e83",
   "metadata": {},
   "source": [
    "`Input`\n",
    " \n",
    "We work with dataset chuncks (or `blocks`).\n",
    "\n",
    "Transformer will never see more than `block_size` when predicting the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f799b61e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 9 chars in train set\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4a7be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) and output is 47\n",
      "when input is tensor([18, 47]) and output is 56\n",
      "when input is tensor([18, 47, 56]) and output is 57\n",
      "when input is tensor([18, 47, 56, 57]) and output is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) and output is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) and output is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) and output is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) and output is 58\n"
     ]
    }
   ],
   "source": [
    "# Each chunk of 9 chars has 8 examples\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"when input is {context} and output is {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72732d9c",
   "metadata": {},
   "source": [
    "Each `4 x 8` input tensor has 32 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9881809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "# how many independent sequences will we process for every fwd, bkwd pass of transfomer \n",
    "batch_size = 4 \n",
    "# what is the maximum context length for predictions\n",
    "block_size = 8 \n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # 4 random start positions  \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('----')\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c714e54a",
   "metadata": {},
   "source": [
    "`Bigram LM`\n",
    " \n",
    "* `Embedding table` encodes the probability of the next char for each char \n",
    "* So, for each input char we just pluck out it's row from the embedding table\n",
    "* That row gives us the raw scores (logits) for the `next char`\n",
    " \n",
    "`Loss`\n",
    " \n",
    "* Apply `softmax` to exp and normalize logits, resulting in a probability  \n",
    "* Then, just fetch the probability of picking the *correct* `next char`\n",
    "* Multiple probabilities (or, in practice, sum the log probabilties) of the correct next char in the batch \n",
    "* If everything is correct, we want `0 loss`, which works out b/c `log(P=1 for correct char) = 0`\n",
    "* This is `negative log liklihood (NLL)`\n",
    "* `Cross entropy loss` just rolls `softmax` normalization and `NLL` into one step \n",
    "* `loss_=F.cross_entropy(logits,Y[ix])` \n",
    "*  `F.cross_entropy` function returns the mean cross-entropy loss over all the elements of the input\n",
    "\n",
    "`Class`:\n",
    "\n",
    "* In the class `BigramLangModel`, the forward method is the one that is being called when `logits, loss = self(idx)` is used in the generate function. \n",
    "* This is because the forward method is defined as the forward pass of the model, which is the default behavior when the `nn.Module` class is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b34413db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "4.725085258483887\n",
      "-4.174387269895637\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLangModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "        # Each token just looks up scores in embedding table\n",
    "        # The scores are the logits for the next char \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # ids and targets are both (B, T) tensor of integers\n",
    "        # B=batch (4), T=time/context (8), C=vocab_size(64)\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # C logits per input char \n",
    "            logits = logits.view(B*T, C)\n",
    "            # 1 output char target \n",
    "            targets = targets.view(B*T)\n",
    "            # For batch of B*T = 4*8 = 32 chars \n",
    "            # Softmax: Compute probability from logits per char \n",
    "            # NLL: Look up probability of the correct char and sum the negative log across all 32 in batch \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # run forward pass to get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution, one prediction for what is next / batch \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLangModel(vocab_size)\n",
    "# Outputs \n",
    "logits, loss = m(xb,yb) \n",
    "print(logits.shape)\n",
    "print(loss.item())\n",
    "print(np.log(1/65))\n",
    "# Generates 100 tokens, untraind model\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d84b10f6",
   "metadata": {},
   "source": [
    "Again the loss is the mean `-log(P_correct_char)`.\n",
    "\n",
    "We expect that it is close to random, or `-log(P=1/65)`.\n",
    "\n",
    "Note, we can easily train. \n",
    "\n",
    "The weights are in the `token_embedding_table` attribute, which is an instance of the `nn.Embedding` module.\n",
    "\n",
    "The embedding layer is initialized with random weights and it is trained during the training process to better represent the input tokens.\n",
    "\n",
    "The `nn.Embedding` module uses these weights to project the input tokens into the continuous vector space.\n",
    " \n",
    "It updates these weights during training to improve the representation of the input tokens, producing raw logits for the next token given the current one.\n",
    "  \n",
    "So the weights are inside the `token_embedding_table` and can be accessed by calling `model.token_embedding_table.weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77890e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3796486854553223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "# Set up training loop \n",
    "batch_size = 32\n",
    "for steps in range(10000): \n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # Evaulate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76501bbe",
   "metadata": {},
   "source": [
    "The model is making progress! \n",
    "\n",
    "We can see the loss is lower than before.\n",
    "\n",
    "Run inference on 100 chars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "69be04f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sel ule-\n",
      "ABOLaig,\n",
      "PUCLEO hoofr rde!\n",
      "Whbodishat, d, igof gr.\n",
      "AMy me atheangrd mbt wofoun dapimurdu's\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4ead3a",
   "metadata": {},
   "source": [
    "But is is only looking at the `last char` to predict the `next char`.\n",
    "\n",
    "We can do better.\n",
    "\n",
    "Let's introduce `self-attention`:\n",
    " \n",
    "We can start with `8 tokens` per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "519479ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1762530",
   "metadata": {},
   "source": [
    "We want the tokens to \"talk\" to each other.\n",
    "\n",
    "In this case, information only flows from prior timesteps:\n",
    "  \n",
    "* E.g., The token in 5th location can only talk to prior tokens. \n",
    " \n",
    "* And to accumulate, we calculate the `average` of the channels in all prior tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02eaa34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"bag of words\" (term for averaging) where each location has a word\n",
    "xbow = torch.zeros((B,T,C))\n",
    "# iterate over batch \n",
    "for b in range(B):\n",
    "    # iterate over time \n",
    "    for t in range(T):\n",
    "        # batch everything up to T (previous chunk)\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8ba65ca",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "* Each token is a char \n",
    "* The model has `block_size = 8`, so it looks at 8 chars\n",
    "* Each char is embedded\n",
    "* So: one example to the model will simply be: `block_size x C` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c11101a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see one example \n",
    "print(x[0].shape)\n",
    "x[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d4005ae",
   "metadata": {},
   "source": [
    "Then, we can see the mean for all tokens up to and including the current in the `bow` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a8b6ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW for the first channel of the second token\n",
    "print(np.mean([0.1808,-0.3596]))\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0ad5424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b= tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c= tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=',a)\n",
    "print('b=',b)\n",
    "print('c=',c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4f8a381",
   "metadata": {},
   "source": [
    "Producing `c` is just dot-product of row 1 `a` and col 1 `b`:  \n",
    "  \n",
    "`[1., 1., 1.]` dot `[2., 6., 6.]` = `14`\n",
    "\n",
    "Note, we can use  `tril` to perform the `BOW` sum for all tokens up to and including the current:\n",
    "\n",
    "Does a sum of a variable number of rows from `b` and deposits into `c`.\n",
    "\n",
    "We can normalize `a` so that the average gets deposited into `c`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51b0a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b= tensor([[7., 6.],\n",
      "        [9., 6.],\n",
      "        [3., 1.]])\n",
      "c= tensor([[7.0000, 6.0000],\n",
      "        [8.0000, 6.0000],\n",
      "        [6.3333, 4.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=',a)\n",
    "print('b=',b)\n",
    "print('c=',c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdafea95",
   "metadata": {},
   "source": [
    "So `b` is our `block_size x C` tensor.\n",
    "\n",
    "We want to accumulate the mean up to each token.\n",
    "\n",
    "`wei` (short for weights) is a mask that enables this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa1d04b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "We can see they are indentical:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So, again we had \"bag of words\" (term for averaging) where each location has a word\n",
    "xbow = torch.zeros((B,T,C))\n",
    "# iterate over batch \n",
    "for b in range(B):\n",
    "    # iterate over time \n",
    "    for t in range(T):\n",
    "        # bach and everything up to T (previous chunk)\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    " \n",
    "# Faster way to do the same thing! \n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "print(wei)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) => B, T, C\n",
    "print(\"We can see they are indentical:\")\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d3cef0a",
   "metadata": {},
   "source": [
    "We are basically doing a weighted sum. \n",
    "\n",
    "The weights come from `T`. \n",
    " \n",
    "They are applied to `x`.\n",
    "\n",
    "So, each element in the output `xbow2` is just the average of all tokens up and including that element.\n",
    "\n",
    "We cam do this a third way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6970df75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "# For all element where tril = 0 become -inf \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# Softmax does a normalization (exp and divide by sum)\n",
    "# Creates the same mask as above\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x # (B, T, T) @ (B, T, C) => B, T, C\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb5a38f",
   "metadata": {},
   "source": [
    "`wei` has interesting properties:\n",
    "\n",
    "* Each value is an interaction strength\n",
    "* How much from each token in the past do we want to aggregate \n",
    "* And we wil not aggregate anything from future tokens\n",
    "* These affinities are going to be learned \n",
    "\n",
    "In short, this is just a weighted aggregated of past token. \n",
    "\n",
    "This is the preview for self-attention!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c18fca2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T x T \n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5599533f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T x C \n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7ec223bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b3c3386",
   "metadata": {},
   "source": [
    "So, the output at `token 2`, for example, will be a weighted aggregated of past tokens.\n",
    "\n",
    "For the first channel:\n",
    "\n",
    "`[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]` \n",
    "\n",
    "`@` \n",
    "\n",
    "`[ 0.1808, -0.3596,  0.6258,  0.9545,  0.3612, -1.3499,  0.2360, -0.9211]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c48f7915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0894"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token 2:\n",
    "0.1808 * 0.5 + -0.3596 * 0.5000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b26366c",
   "metadata": {},
   "source": [
    "We can see this is in the output, `-0.0894`.\n",
    "\n",
    "So, in this simple case: we equally aggerate (`0.5`) from token 1 and 2 for the output at token 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3130f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: train loss 1.0000, val loss 1.0000\n"
     ]
    }
   ],
   "source": [
    "iter = 1\n",
    "losses={}\n",
    "losses['train']=1\n",
    "losses['val']=1\n",
    "print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cadf4c3e",
   "metadata": {},
   "source": [
    "`Self-attention`\n",
    "\n",
    "Start with a 4x8 arrangement of tokens. \n",
    "\n",
    "This computes an average of all past tokens to compute the logit for current token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1fcc5288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x=torch.rand(B,T,C)\n",
    "# mask \n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# weight matrix, uniformly default to zero \n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "out.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e76e7982",
   "metadata": {},
   "source": [
    "Now, the weight matrix is uniformly set to zero.\n",
    "\n",
    "So, each token pays equal attention to each prior token.\n",
    "\n",
    "But, we want select information from past in the data-dependent way.\n",
    " \n",
    "`Self-attention`\n",
    " \n",
    "`Query`: what am I looking for.\n",
    "`Key`: what do I contain.\n",
    "\n",
    "Each query does a dot-product with each prior location.\n",
    "\n",
    "If a `Key` and `Query` are aligned, they will produce a high value.\n",
    "\n",
    "Then, I will learn more about that token!\n",
    "\n",
    "We simply specify a `head_size` for this self-attention layer.\n",
    "\n",
    "Then, we forward key and query on `x`: all tokens in every position on `B x C` produce a key and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4661bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_size = 16\n",
    "\n",
    "# Input \n",
    "B,T,C = 4,8,32\n",
    "x=torch.rand(B,T,C)\n",
    " \n",
    "# Per token embeddings\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "# Apply to all tokens \n",
    "k = key(x) # B, T, 16\n",
    "q = query(x) # B, T, 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5fd50e9",
   "metadata": {},
   "source": [
    "So, we have a B, T tensor of tokens.\n",
    "\n",
    "We embed each to a Key and Query.\n",
    "\n",
    "We then let the Key and Query communicate across tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3687b720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Weight, measuring affinity between tokens \n",
    "wei = q @ k.transpose(-2,-1) * head_size**0.5 # (B,T,16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "# Mask future tokens since we are decoding \n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# V gets aggregated for this self-attention head\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "out.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65a5896e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d14c2d1c",
   "metadata": {},
   "source": [
    "Weight is now `T x T`\n",
    "\n",
    "The attention that token N pays to prior tokens is learned by data.\n",
    "\n",
    "Specifically, it is learned by the dot product of `k`, `q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9cdf70ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2924,  2.7755,  1.7500,  2.1041,  1.1474,  2.4048,  0.1968,  1.3166],\n",
       "        [ 4.6807,  5.0168,  3.5054,  3.3999,  3.2272,  5.0683,  1.7219,  2.1858],\n",
       "        [ 3.4592,  3.0671,  2.8662,  2.3773,  2.2361,  3.1440,  1.5106,  1.1534],\n",
       "        [ 3.4917,  2.5129,  2.2739,  2.0483,  2.1242,  3.0338,  0.8786,  0.1875],\n",
       "        [ 3.8347,  3.7708,  3.5620,  2.4863,  2.4752,  4.8359,  0.8099,  1.4257],\n",
       "        [ 4.1176,  3.5148,  2.5157,  2.0536,  1.9548,  3.4721,  1.4021,  0.5436],\n",
       "        [ 2.7520,  1.7684,  2.0399,  1.5793,  1.4491,  2.5530,  0.4618,  0.0639],\n",
       "        [ 1.3894,  1.9540,  1.2960,  0.9153,  1.1477,  1.7391, -0.1393, -0.2193]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = q @ k.transpose(-2,-1) * head_size**0.5\n",
    "wei[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b78b699",
   "metadata": {},
   "source": [
    "Attention is a general message passing mechanism for any directed graph.\n",
    " \n",
    "E.g., - \n",
    "\n",
    "`token 8` is pointed to by itself, and all 7 prior nodes.\n",
    "\n",
    "so, it can aggregate information from all nodes that point to it!\n",
    "\n",
    "`x` is private information to `token 8`.\n",
    "\n",
    "`q` is what `token 8` is interested in.\n",
    "\n",
    "`k` is what `token 8` has.\n",
    "\n",
    "`v` is what `token 8` will communicate to you if you find it interesting.\n",
    "\n",
    "Strong attention pairs have high value of `Q @ K`! \n",
    "\n",
    "Also, there is no notion of space by default; this is encoded.\n",
    "\n",
    "Also, there is no communcation across nodes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c01aa749",
   "metadata": {},
   "source": [
    " `Encoder vs Decoder`\n",
    "\n",
    "`decoder`\n",
    "\n",
    "Typically just self-attention (communication) and feed-forward (compute).\n",
    "\n",
    "Uses triangular mask on future tokens.\n",
    " \n",
    "It has an auto-regressive property where we can sample from it.\n",
    "\n",
    "The example here is decoder only.\n",
    "\n",
    "`encoder`\n",
    "\n",
    "What if we want to `condition` the decoding (above) on additional information?\n",
    "\n",
    "If so, we can add an encoder.\n",
    "\n",
    "E.g., an encoder reads French and creates tokens. All tokens can talk! No masking.\n",
    "\n",
    "The encoder feeds `k`, `v` to the decoder via `cross-attention`.\n",
    "\n",
    "Self-attention: the same source X produces `K`,`Q`,`V`\n",
    "\n",
    "But, the `Q` can come from X while `K`,`V` come from a seperate source.\n",
    "\n",
    "With `cross-attention`, we pool from a seperate source of nodes.\n",
    "\n",
    "So, we condition decoding on:\n",
    "\n",
    "1/ The past (as normal)\n",
    "\n",
    "2/ The full encoded input sentence (e.g., in French)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "563005e9",
   "metadata": {},
   "source": [
    "`Multi-head attention`\n",
    "\n",
    "Compute many attentions in parallel. \n",
    "\n",
    "Basically, we create many channels of communication between the tokens. \n",
    "\n",
    "Concatenate the results. \n",
    "\n",
    "`Feed Forward`\n",
    "\n",
    "Self-attention is the communication.\n",
    "\n",
    "FF is the \"thinking\" or computing of this data.\n",
    " \n",
    "It is common for a FF layer on each token independently. \n",
    "\n",
    "Self-attention (communication) and compute blocks are stacked.\n",
    "\n",
    "`Overfitting`\n",
    "\n",
    "Two innovations help with large networks.\n",
    " \n",
    "`1. Skip (residual) connections`\n",
    "\n",
    "We use this to improve generalization. \n",
    "\n",
    "We bypass computation and sum.\n",
    "\n",
    "Addition distributes gradients equally during backprop.\n",
    "\n",
    "At the start of training, residual blocks to do not contribute to gradient. \n",
    "\n",
    "So, there is a gradient superhighway back to the input.\n",
    " \n",
    "But, over time, residual blocks kick in. \n",
    "\n",
    "`2. Layer norm`\n",
    "\n",
    "Very similar to batch norm, which ensures each neuron has unit Gaussian output.\n",
    "\n",
    "`3. Dropout`\n",
    " \n",
    "Regularization technique for prevent overfitting.\n",
    "\n",
    "Effectivly, this trains an ensemble of sub-netoworks. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fc85ba8",
   "metadata": {},
   "source": [
    "Model size - \n",
    "\n",
    "Pretraining:\n",
    "\n",
    "Ours: 10M params and dataset is ~300k tokens (chars)\n",
    "    \n",
    "OAI: Up to 175B (96 heads, 128 head size) using sub-word chunks (~50k vocab) and 300b tokens.\n",
    "\n",
    "This is document completing decoder model, babbling \"internet\" rather than Shakespeare. \n",
    "\n",
    "It is not aligned.\n",
    "\n",
    "Fine-tuning:\n",
    "\n",
    "Very sample effieicnt in fine-tuning.\n",
    "\n",
    "In the second stage, we align it:\n",
    "\n",
    "`SFT:` Q and A documents. ~Thousands. Align model to expect question and complete answer. \n",
    "\n",
    "`RLFH` Ranked responses. Predict how much of any response would be desirable via reward model. Then run PPO to fine-tune sampling policy. Answers expect to have high reward.\n",
    "\n",
    "Thus, the model goes from a document completer to a question-answer-er.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7a78e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b08da8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
